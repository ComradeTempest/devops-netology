1. Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её нужно прервать.

Что делаем? Во-первых, пуляем в монгу запрос на отображение operation id с временем исполнения > 180 секунд. Дальше прибиваем операцию через db.killOp
Что делаем, чтобы предотвратить такое в дальнейшем? Первое, это проанализировать построение запроса с помощью executionStats, вдруг он там сам по себе дичь творит?
Если он не творит дичь, то мягко ограничиваем запрос по лимиту исполнения методом maxTimeMS. Если и дальше всё не так и всё не то, то накидываем половинку 
имеющихся ресурсов, иногда творит чудеса. А если и так скрипит, то ставим редис или квинту — там другие баги.

2. Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. Причем отношение количества записанных key-value значений к 
количеству истёкших значений есть величина постоянная и увеличивается пропорционально количеству реплик сервиса.

Что с редиской? Обычно такое бывает, когда редиске не хватает памяти — она забивается expired ключами и либо ломается механизм размещения нового потока (ибо некуда),
либо редис начинает чиститься и тоже уходит на это время в отказ. Решение: не мучить сервер и подать ему ещё памяти и подкрутить в конфиге redis.conf количество 
expire-циклов (подгрузит проц, но экспайры будут разгребаться сообразно быстрее).

3. Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы, пользователи начали жаловаться на ошибки вида:
InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '

Наперво проверить сеть. Можно почесать дамп, например, ваершарка. Если сеть великолепно работает, то анализируем запрос на предмет титанических размеров.
Если запрос скромный, то выставляем удвоенные лимиты в конфиге на таймауты ожидания, сетевой записи и чтения, а так же на максимальный дозволенный 
размер пакета, как на стороне сервера, так и клиента. Проверяем. Если ошибка на месте, удваиваем лимиты повторно. Если после этого ошибка исчезает, то
потихоньку откручиваем параметры поштучно до её появления (таким образом, конкретизируем проблему). Если ошибка остаётся, переходим на Постгрес.

4. Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с большим объемом данных лучше, чем MySQL.
После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:
postmaster invoked oom-killer

Тут проблема чистая и ясная — постгрес объелся памяти и начал потихоньку подпирать ядро. Тут происходит либо краш системы, либо oom-killer режет процесс, который
отжал всю память. Итак, перед нами два пути: первый это немножко выстрелить себе в ногу и выставить sudo -s sysctl -w vm.oom-kill = 0 с параллельными настройками
оверкоммита памяти и тогда постгрес будет уходить в своп (обычно свопа хватает, пока на oom-алерт не среагирует техник и не прибежит чинить). Второй путь чуть
более заморочный, но определённо, рекоммендуемый — проверить через, например, htop конкурентов в пищевой цепочке потребителей памяти на предмет таких же любителей памяти,
накинуть ещё RAM, ну и наконец, оптимизировать запросы и отладить постгрес через конфиг, подкрутив ему количество соединений, буфер, кэш и т.д.


